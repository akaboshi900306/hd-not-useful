{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install RISE\n",
    "!pip install --upgrade google-api-core\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install --upgrade google-cloud-bigquery\n",
    "!pip install --upgrade grpcio\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced Python\n",
    "\n",
    " ![Python](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/600px-Python-logo-notext.svg.png)\n",
    "\n",
    "## Rich Conboy & Kevin Kelleher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ![Python](https://www.python.org/static/community_logos/python-logo-master-v3-TM.png)\n",
    "\n",
    "# What this class is\n",
    " - Tips and tricks to improve your experience working with Python at The Home Depot\n",
    " - Strategies for speeding up workflows you may already be using\n",
    " \n",
    "# What this class is not\n",
    " - Intro to programming/python\n",
    " - Data mining / machine learning tutorials (go see Matt Morton's class!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "- JupyterHub Basics\n",
    "- Faster downloads with Pandas + BigQuery Storage API\n",
    "- Python Productivity\n",
    "- UI Design with ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What the heck is JupyterHub and why do I care?\n",
    "![Logo](https://www.dataquest.io/wp-content/uploads/2019/01/1-LPnY8nOLg4S6_TG0DEXwsg-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Jupyter Notebook** is a web application that allows the end-user to create documents that contain live code (in python, R, or Julia), equations, visualizations and narrative Markup Markdown text.  Jupyter Notebooks are used for data cleansing, data transformation, statistical modeling, data visualization, and machine learning.\n",
    "\n",
    "![Jupyter Notebook](https://www.dataschool.io/content/images/2019/03/binder-50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**JupyterHub** is a multi-user server that manages and proxies multiple instances of a single-user of JupyterLab or Jupyter Notebooks.  JupyterHub is the best way to serve Jupyter notebook for multiple users. It can be used in a classes of students, a corporate data science group or scientific research group.  It is a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server.\n",
    "\n",
    "![JupyterHub Architecture](https://jupyterhub.readthedocs.io/en/stable/_images/jhub-fluxogram.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using JupyterHub\n",
    "\n",
    "### Getting Access:\n",
    "- **Required:** GG_CLOUD_GCP_your-cloud-project-id_DATA_ANALYST or GG_CLOUD_GCP_your-cloud-project-id_DATA_SCIENTIST (to get read / write access to your team's business project)\n",
    "- **Required:** GG_CLOUD_EDW_DATA_USER (to get read access to the data in the IT supported pr- projects)\n",
    "- **Required:** GG_CLOUD_GCP_your-cloud-project-id_DATALAB_USER (to get access to JupyterHub)\n",
    "- **Optional:** GG_CLOUD_GCP_your-cloud-project-id_DATALAB_POWERUSER in addition to GG_CLOUD_GCP_your-cloud-project-id_DATALAB_USER (to spawn medium or large size servers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using JupyterHub\n",
    "\n",
    "### Pricing:\n",
    "Once you spawn a server, there is a charge for the first minute and then every second after that.  **Best practice is to use the smallest instance possible for your use case.**  Start with the small server and move up if necessary.\n",
    "\n",
    "| Spawner Selection   | Cores and RAM                | Cost               |\n",
    "| :------------------ | :--------------------------: | :----------------: |\n",
    "| Small Instance      | 2 cores up to 13 gb of RAM   | \\$0.19 per hour    |\n",
    "| Medium Instance     | 8 cores up to 52 gb of RAM   | \\$0.76 per hour    |\n",
    "| Large Instance      | 64 cores up to 416 gb of RAM | \\$4.56 per hour    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using JupyterHub\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "|                                                                             | JupyterHub    | Local Workstation |\n",
    "| :-------------------------------------------------------------------------- | :-----------: | :---------------: |\n",
    "| Flexible computing power and memory options (up to 64 cores & 416 GB RAM)   | &#10004;      |                   |\n",
    "| No fiddling with Google Cloud SDK, gsutil, or security certificates         | &#10004;      |                   |\n",
    "| Super fast download to Pandas DataFrame via Cloud Storage API               | &#10004;      |                   |\n",
    "| Package management directly in notebooks preserves compatibility            | &#10004;      |                   |\n",
    "| Free                                                                        |               | &#10004;          |\n",
    "| Python virtual environment flexibility via Anaconda                         |               | &#10004;          |\n",
    "| Zero downtime or maintenance issues                                         |               | &#10004;          |\n",
    "| Interface with on-prem data (SQL Server anyone?) and local tools            |               | &#10004;          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fast download to Pandas via Cloud Storage API\n",
    "\n",
    "One of the most common things we find ourselves doing anytime we are working with data in Python is downloading it from BigQuery.  Historically, this is probably one of the biggest pain points we've seen in the Home Depot environment.  I'll bet most of the people here have run some flavor of the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sql = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `analytics-supplychain-thd.Rich.ALLOC_HIST`\n",
    "    LIMIT 100000\n",
    "\"\"\"\n",
    "\n",
    "# Run a Standard SQL query with the project set explicitly\n",
    "df = pd.read_gbq(sql, project_id='analytics-supplychain-thd', dialect='standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can do better by adding a couple of optional components of google-cloud-bigquery..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade google-cloud-bigquery[bqstorage,pandas,pyarrow]\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sql = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `analytics-supplychain-thd.Rich.ALLOC_HIST`\n",
    "    LIMIT 100000\n",
    "\"\"\"\n",
    "\n",
    "# Run a Standard SQL query with the project set explicitly\n",
    "df = pd.read_gbq(sql, project_id='analytics-supplychain-thd', dialect='standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python Productivity\n",
    "<p>In this section, we'll walk through an example problem and show some tips and tricks along the way to make you more productive and make your code faster and more accurate.</p>\n",
    "<p>The problem: we have a list of POs ranked by priority, and we have to choose which POs to include on the next truck while making sure the truck is as close to 2,400 cube as possible. We've also just been given a list of priority SKUs, so first we need to update the existing priority of the POs if it contains any of the priority SKUs.</p>\n",
    "<p>First, we use pandas to pull in our data and print out the first five rows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sku_df = pd.read_gbq(\n",
    "'''\n",
    "SELECT * FROM `analytics-supplychain-thd.Kevin.SOD_SKUS`\n",
    "'''\n",
    "    , dialect='standard'\n",
    "    , project_id='analytics-supplychain-thd'\n",
    ")\n",
    "\n",
    "po_df = pd.read_gbq(\n",
    "'''\n",
    "SELECT * FROM `analytics-supplychain-thd.Kevin.SOD_POS`\n",
    "'''\n",
    "    , dialect='standard'\n",
    "    , project_id='analytics-supplychain-thd'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sku_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "po_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>Below is our first attempt at updating the PO priority based on our priority SKU list. We use the pandas apply function to call our update_priority function on each row. We loop through each priority SKU, and check if it's included on the PO. If it is, we add 50,000 to the existing priority score. Otherwise, we return the original priority. Let's test it out on the first 1,000 POs:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "priority_skus = list(sku_df['SKU_NBR'])\n",
    "\n",
    "def update_priority(row):\n",
    "    for priority_sku in priority_skus:\n",
    "        if str(priority_sku) in row['SKUS']:\n",
    "            return row['PRIORITY'] + 50000\n",
    "    return row['PRIORITY']\n",
    "    \n",
    "po_df.head(1000).apply(update_priority, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>23 seconds for only 1,000 rows?? At this rate, it will take an hour to update all of our data!</p>\n",
    "<p>Our first takeaway here should be to <strong>use SQL whenever possible</strong> - SQL joins are very optimized and prevent exactly the issue we have here.</p>\n",
    "<p>Sometimes that's not an option though, so let's see if we can speed up our code. An easy option is to use the %%prun magic, which creates a profile of which operations in your code are taking the longest.</p>\n",
    "<p>In our case, the profile isn't very helpful, but if your code is more complex than our example here, it can help point you in the right direction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%prun\n",
    "\n",
    "priority_skus = list(sku_df['SKU_NBR'])\n",
    "\n",
    "def update_priority(row):\n",
    "    for priority_sku in priority_skus:\n",
    "        if str(priority_sku) in row['SKUS']:\n",
    "            return row['PRIORITY'] + 50000\n",
    "    return row['PRIORITY']\n",
    "    \n",
    "po_df.head(1000).apply(update_priority, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>If we think about our code, we can see that for each row in po_df (~150K rows), we evaluate each priority SKU (~4K SKUs), which means we're attempting to perform ~600 million operations - no wonder the code is slow!</p>\n",
    "<p>If we google something like \"python fastest way to check if element in list,\" we may get the idea to try using sets rather than lists. Let's try it out!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "priority_skus = list(sku_df['SKU_NBR'])\n",
    "\n",
    "priority_skus_set = set(sku_df['SKU_NBR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "1 in priority_skus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "1 in priority_skus_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>After doing some googling on what exactly these units are, you'll see that using the set provides a 1,000x speedup!</p>\n",
    "<p>To check if an item is in a list, we have to iterate through each element in this list.</p>\n",
    "<p>On the other hand, you can think of the set as having an index - we can immediately determine whether or not the element is in the set. Python dictionaries also have this property.</p>\n",
    "<p>Let's try rewriting our original code to use sets:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "priority_skus_set = set(str(x) for x in list(sku_df['SKU_NBR']))\n",
    "\n",
    "def update_priority(row):\n",
    "    skus = set(row['SKUS'].split(','))\n",
    "    if not skus.isdisjoint(priority_skus_set):\n",
    "        return row['PRIORITY'] + 50000\n",
    "    return row['PRIORITY']\n",
    "    \n",
    "po_df.apply(update_priority, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>We just ran our code for every row (not just the first 1,000) in 3 seconds - again, a 1,000x speedup!</p>\n",
    "<p>We forgot to save our results, but there is a trick that will prevent you from having to rerun your code. Jupyter notebook automatically saves the results from the last cell run in a variable named \"_\" - we can assign this as a column in our data frame.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "po_df['NEW_PRIORITY'] = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "po_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>Now that our PO priorities are updated, we can return to the original problem: for each store, send the highest priority POs on the next truck while filling the truck as close to 2,400 cube as possible.</p>\n",
    "<p>Below is our first attempt - we use pandas groupby-apply to separate our data for each store and apply the build_truck function separately. We sort the data by highest priority, then loop through the POs. For each PO we encounter, we check if adding it will put us over 2,400 cube - if it won't, we add it, otherwise we move on to the next PO:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "po_df['INCLUDED'] = False\n",
    "\n",
    "def build_truck(g):\n",
    "    included_cube = 0\n",
    "    g.sort_values('NEW_PRIORITY', ascending=False, inplace=True)\n",
    "    for i, row in g.iterrows():\n",
    "        if included_cube <= 2400:\n",
    "            g.loc[i, 'INCLUDED'] = True\n",
    "            included_cube += row['TTL_CUBE']\n",
    "    return g\n",
    "\n",
    "new_po_df = po_df.groupby('STR_NBR', as_index=False).apply(build_truck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>No major performance issues here! Let's check our results. Below is an assert statement that checks to see that the included POs for each store total to less than 2,400 cube. The assert statement will throw an error if our test returns False.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assert (new_po_df[new_po_df['INCLUDED']].groupby('STR_NBR')['TTL_CUBE'].sum() <= 2400).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>That's not good! Good thing we checked our logic before sending out the results.</p>\n",
    "<p>We can add an assert statement to our original code and use the %debug magic to get a better sense of what's going on - this is a really powerful combination that lets you see exactly what's going on inside your functions. The %debug magic lets us step through the code and print all of our variables at the time of the last error - much faster than adding print statements one by one.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "po_df['INCLUDED'] = False\n",
    "\n",
    "def build_truck(g):\n",
    "    included_cube = 0\n",
    "    g.sort_values('NEW_PRIORITY', ascending=False, inplace=True)\n",
    "    for i, row in g.iterrows():\n",
    "        if included_cube <= 2400:\n",
    "            g.loc[i, 'INCLUDED'] = True\n",
    "            included_cube += row['TTL_CUBE']\n",
    "        assert included_cube <= 2400\n",
    "    return g\n",
    "\n",
    "new_po_df = po_df.groupby('STR_NBR', as_index=False).apply(build_truck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check out the debugger commands <a href=\"https://docs.python.org/3/library/pdb.html#debugger-commands\">here.</a><br>\n",
    "These take some getting used to, but are well worth taking the time to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>Ah, we're only checking if included cube is under 2,400 before adding a PO, not checking if adding the PO will push us over 2,400. Let's update the code and make sure our assert statements don't throw any errors after doing so:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "po_df['INCLUDED'] = False\n",
    "\n",
    "def build_truck(g):\n",
    "    included_cube = 0\n",
    "    g.sort_values('NEW_PRIORITY', ascending=False, inplace=True)\n",
    "    for i, row in g.iterrows():\n",
    "        if included_cube + row['TTL_CUBE'] <= 2400:\n",
    "            g.loc[i, 'INCLUDED'] = True\n",
    "            included_cube += row['TTL_CUBE']\n",
    "        assert included_cube <= 2400\n",
    "    return g\n",
    "\n",
    "new_po_df = po_df.groupby('STR_NBR', as_index=False).apply(build_truck)\n",
    "\n",
    "assert (new_po_df[new_po_df['INCLUDED']].groupby('STR_NBR')['TTL_CUBE'].sum() <= 2400).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p>Now we're done! And if we have to perform this analysis again or on a regular basis, our assert statements will give us added confidence that our code continues to give correct results.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# UI Design with ipywidgets\n",
    "\n",
    "**ipywidgets** is a neat module that let's you construct mini-applications using Jupyter notebook as a front-end.\n",
    "\n",
    "### Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "# Create Widgets\n",
    "name = widgets.Text(description='Name: ')\n",
    "print_name = widgets.Button(description='Click me!')\n",
    "\n",
    "# Define Button Action\n",
    "print_name.on_click(lambda _: print('Hello {}!'.format(name.value)))\n",
    "\n",
    "# Display UI\n",
    "display(name)\n",
    "display(print_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A more complicated example using conditional logic..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create Widgets\n",
    "ds_toggle = widgets.Checkbox(value=False, description='Please check if you work in Supply Chain', disabled=False, indent=False)\n",
    "ds_select = widgets.Dropdown(options=['Kevin', 'Samantha', 'Megan', 'Rich', 'Obi'], description='Who is the best Data Science Team Member: ', disabled=False, indent=False, layout=widgets.Layout(width='100%'), style={'description_width': 'initial'})\n",
    "\n",
    "# Display First Widget\n",
    "display(ds_toggle)\n",
    "\n",
    "# Create Output For Interactive Widgets\n",
    "ds_output = widgets.Output()\n",
    "display(ds_output)\n",
    "\n",
    "# Code to Print the Best\n",
    "def on_select_change(change):\n",
    "    ds_output.clear_output()\n",
    "    with ds_output:\n",
    "        display(ds_select)\n",
    "        if ds_select.value != 'Rich':\n",
    "            print(ds_select.value, 'is a good choice!')\n",
    "        else:\n",
    "            print('Please select again')\n",
    "\n",
    "# Code to Make Confidence Level Show Up When Mahalanobis Toggle is Checked\n",
    "def on_toggle_change(change):\n",
    "    if ds_toggle.value == True:\n",
    "        with ds_output:\n",
    "            display(ds_select)\n",
    "    else:\n",
    "        ds_output.clear_output()\n",
    "        \n",
    "ds_select.observe(on_select_change, names='value')\n",
    "ds_toggle.observe(on_toggle_change, names='value')\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
